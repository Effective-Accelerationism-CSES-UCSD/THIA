{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a21b8e",
   "metadata": {},
   "source": [
    "## Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27577069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\Lib\\site-packages\\accelerate\\utils\\modeling.py:808: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d885460f5f34e3f9ba25e4e96ea5459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Mistral-7B-Instruct-v0.3\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# tokenizer and model（automatically use FP16; active GPU）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c73c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0.dev20250701+cu128\n",
      "True\n",
      "NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the prompt\n",
    "with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47315cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and saved.\n"
     ]
    }
   ],
   "source": [
    "response_only = True\n",
    "\n",
    "# Output the answer\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "if response_only:\n",
    "    response = response[len(prompt):].strip()\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "print(\"Generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b293190",
   "metadata": {},
   "source": [
    "## LLM with Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313bc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from https://huggingface.co/datasets/Amod/mental_health_counseling_conversations\n",
    "# And take a look\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1cdc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Context', 'Response']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971436c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)\n",
    "print(hasattr(faiss, 'StandardGpuResources'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ee4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4029e3b18240be946239eedb7fc50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import re, torch, faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# all-MiniLM-L6-v2\n",
    "\n",
    "# 1. Only keep those contain links\n",
    "ds = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")\n",
    "ds = ds.filter(lambda x: \"http\" in x[\"Response\"])\n",
    "\n",
    "questions = [r[\"Context\"].strip() for r in ds]\n",
    "answers   = [r[\"Response\"].strip() for r in ds]   # align the index\n",
    "\n",
    "# 2. question embedding & FAISS\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "q_vecs = embedder.encode(questions, normalize_embeddings=True, batch_size=64)\n",
    "index  = faiss.IndexFlatIP(q_vecs.shape[1])\n",
    "index.add(q_vecs)\n",
    "\n",
    "# 3. LLM\n",
    "tok = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(query: str, max_new: int = 256) -> str:\n",
    "    qv = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)\n",
    "    _, idx = index.search(qv, 1)            # top-1\n",
    "    context = answers[int(idx[0][0])]       # Full answer with the link\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an empathetic mental-health assistant. Use kind language. No need to repeat the question.\\n\\n\"\n",
    "        f\"Knowledge (you should use the full link it contains):\\n{context}\\n\\n\"\n",
    "        f\"User: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    with open(\"prompt_rag.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    ans_full = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return ans_full[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb21514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Check prompt_rag.txt and output.txt.\n"
     ]
    }
   ],
   "source": [
    "# 5. Main: Read prompt.txt → Write output.txt\n",
    "# Every winter I find myself getting sad because of the weather. How can I fight this?\n",
    "# Alt: The bright, warm summer always seem to bring my mood down. I am crazy!! I love eating pizza!!!! But how can I push back against that? Can I eat more pizza?\n",
    "\n",
    "with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_q = f.read().strip()\n",
    "\n",
    "answer = rag_chat(user_q)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(answer)\n",
    "\n",
    "print(\"Done! Check prompt_rag.txt and output.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc227b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
