{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a21b8e",
   "metadata": {},
   "source": [
    "## Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27577069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\lib\\site-packages\\accelerate\\utils\\modeling.py:808: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe5104eb948469b955049deb91a2d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Mistral-7B-Instruct-v0.3\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# tokenizer and model（automatically use FP16; active GPU）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970c73c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0.dev20250707+cu128\n",
      "True\n",
      "NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa19949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the prompt\n",
    "with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47315cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and saved.\n"
     ]
    }
   ],
   "source": [
    "response_only = True\n",
    "\n",
    "# Output the answer\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "if response_only:\n",
    "    response = response[len(prompt):].strip()\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "print(\"Generated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b293190",
   "metadata": {},
   "source": [
    "## LLM with Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313bc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Amod/mental_health_counseling_conversations couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Laplace\\.cache\\huggingface\\datasets\\Amod___mental_health_counseling_conversations\\default\\0.0.0\\4672e03c7f1a7b2215eb4302b83ca50449ce2553 (last modified on Wed Jul  2 18:13:28 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from https://huggingface.co/datasets/Amod/mental_health_counseling_conversations\n",
    "# And take a look\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1cdc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Context', 'Response']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971436c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)\n",
    "print(hasattr(faiss, 'StandardGpuResources'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ee4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Amod/mental_health_counseling_conversations couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Laplace\\.cache\\huggingface\\datasets\\Amod___mental_health_counseling_conversations\\default\\0.0.0\\4672e03c7f1a7b2215eb4302b83ca50449ce2553 (last modified on Wed Jul  2 18:13:28 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e8b50513c54d75b46d78569a23970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1457d42c20064f98aa0d8db8cce9d346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import re, torch, faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# all-MiniLM-L6-v2\n",
    "\n",
    "# 1. Only keep those contain links\n",
    "ds = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")\n",
    "ds = ds.filter(lambda x: \"http\" in x[\"Response\"])\n",
    "\n",
    "questions = [r[\"Context\"].strip() for r in ds]\n",
    "answers   = [r[\"Response\"].strip() for r in ds]   # align the index\n",
    "\n",
    "# 2. question embedding & FAISS\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "q_vecs = embedder.encode(questions, normalize_embeddings=True, batch_size=64)\n",
    "index  = faiss.IndexFlatIP(q_vecs.shape[1])\n",
    "index.add(q_vecs)\n",
    "\n",
    "# 3. LLM\n",
    "tok = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7abe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(query: str, max_new: int = 128) -> str: #256\n",
    "    qv = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)\n",
    "    _, idx = index.search(qv, 1)            # top-1\n",
    "    context = answers[int(idx[0][0])]       # Full answer with the link\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an empathetic mental-health assistant. Use kind language and complete sentence. No need to repeat the question.\\n\\n\"\n",
    "        f\"Knowledge (you should use the full link it contains):\\n{context}\\n\\n\"\n",
    "        f\"User: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    with open(\"prompt_rag.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    ans_full = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return ans_full[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb21514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Check prompt_rag.txt and output.txt.\n"
     ]
    }
   ],
   "source": [
    "# 5. Main: Read prompt.txt → Write output.txt\n",
    "# Every winter I find myself getting sad because of the weather. How can I fight this?\n",
    "# Alt: The bright, warm summer always seem to bring my mood down. I am crazy!! I love eating pizza!!!! But how can I push back against that? Can I eat more pizza?\n",
    "\n",
    "with open(\"prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_q = f.read().strip()\n",
    "\n",
    "answer = rag_chat(user_q)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(answer)\n",
    "\n",
    "print(\"Done! Check prompt_rag.txt and output.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc227b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Version: 1.7.2\n",
      "FAISS has GPU support: True\n",
      "\n",
      "PyTorch CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce RTX 5080\n",
      "CUDA version (from PyTorch): 12.8\n",
      "\n",
      "nvcc version output:\n",
      " nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"FAISS Version:\", faiss.__version__)\n",
    "print(\"FAISS has GPU support:\", hasattr(faiss, 'StandardGpuResources'))\n",
    "\n",
    "print(\"\\nPyTorch CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version (from PyTorch):\", torch.version.cuda)\n",
    "\n",
    "try:\n",
    "    # Check system CUDA version from nvcc\n",
    "    output = subprocess.check_output(['nvcc', '--version']).decode()\n",
    "    print(\"\\nnvcc version output:\\n\", output)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n`nvcc` not found in PATH — you may not have installed CUDA toolkit properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97472656",
   "metadata": {},
   "source": [
    "## Experinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad2e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\lib\\site-packages\\accelerate\\utils\\modeling.py:808: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f7e8be53c84af3ad0e6d7bb749e803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Model & tokenizer (simply same)\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          TextIteratorStreamer)\n",
    "import torch, time, threading, sys\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "_ = llm.generate(**tok(\"hi\", return_tensors=\"pt\").to(llm.device), max_new_tokens=1) # Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a1c20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laplace\\AppData\\Local\\Temp\\ipykernel_20140\\2486641462.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Laplace\\miniconda3\\envs\\mistral7b\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Laplace\\AppData\\Local\\Temp\\ipykernel_20140\\2486641462.py:22: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=5, ai_prefix=\"AI\", human_prefix=\"User\")\n"
     ]
    }
   ],
   "source": [
    "# Build / load FAISS (LangChain) on questions\n",
    "from datasets import load_dataset\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "if not Path(\"qa_faiss.faiss\").exists():\n",
    "    ds = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")\n",
    "    ds = ds.filter(lambda x: \"http\" in x[\"Response\"])\n",
    "    docs = [Document(page_content=q.strip(), metadata={\"answer\": a.strip()})\n",
    "            for q, a in zip(ds[\"Context\"], ds[\"Response\"])]\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(docs, emb)\n",
    "    vectordb.save_local(\".\", index_name=\"qa_faiss\")\n",
    "else:\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.load_local(\".\", emb, index_name=\"qa_faiss\",\n",
    "                                allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})\n",
    "memory = ConversationBufferWindowMemory(k=5, ai_prefix=\"AI\", human_prefix=\"User\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572913b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming chat\n",
    "def stream_chat_from_txt(max_new=64, temp=0.7):\n",
    "    # read user prompt\n",
    "    user_q = Path(\"prompt.txt\").read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "    '''\n",
    "    You are a compassionate and conversational therapist trained in active listening. \n",
    "    You ask thoughtful, open-ended questions and build emotional rapport. \n",
    "    Use empathy, paraphrasing, and gentle nudging. \n",
    "    '''\n",
    "\n",
    "    history_block = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    if history_block:\n",
    "        history_block = \"Conversation History:\\n\" + history_block + \"\\n\\n\"\n",
    "\n",
    "\n",
    "    # RAG & timing\n",
    "    t_rag_start = time.time()\n",
    "    doc = retriever.get_relevant_documents(user_q)[0]\n",
    "    ctx  = doc.metadata[\"answer\"]\n",
    "    full_prompt = (\n",
    "        \"You are an empathetic mental-health assistant. \"\n",
    "        \"Use kind language and complete sentences. \"\n",
    "        \"Do not repeat the question. \\n\\n\"\n",
    "        f\"{history_block}\"\n",
    "        f\"Knowledge you can refer to if you need:\\n{ctx}\\n\\n\"\n",
    "        f\"User: {user_q}\\nAnswer:\"\n",
    "    )\n",
    "    rag_latency = time.time() - t_rag_start\n",
    "    print(f\"\\nRAG latency: {rag_latency:.2f} s\")\n",
    "    Path(\"prompt_rag.txt\").write_text(full_prompt, encoding=\"utf-8\")\n",
    "\n",
    "    # generate (stream, first-token timer)\n",
    "    streamer = TextIteratorStreamer(tok, skip_prompt=True, skip_special_tokens=True)\n",
    "    inputs   = tok(full_prompt, return_tensors=\"pt\").to(llm.device)\n",
    "\n",
    "    threading.Thread(\n",
    "        target=llm.generate,\n",
    "        kwargs=dict(**inputs, streamer=streamer,\n",
    "                    max_new_tokens=max_new,\n",
    "                    temperature=temp,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tok.eos_token_id),\n",
    "        daemon=True\n",
    "    ).start()\n",
    "\n",
    "    t0 = time.time(); first=True; answer=\"\"\n",
    "    for chunk in streamer:\n",
    "        if not chunk:\n",
    "            continue\n",
    "        if first:\n",
    "            print(f\"\\nFirst token latency: {time.time()-t0:.2f} s\\n\")\n",
    "            first = False\n",
    "        sys.stdout.write(chunk); sys.stdout.flush()\n",
    "        answer += chunk\n",
    "    gen_time = time.time() - t0\n",
    "    n_tokens = len(tok.encode(answer, add_special_tokens=False))\n",
    "    print(f\"\\n\\nTokens per second: {n_tokens/gen_time:.2f}\")\n",
    "    \n",
    "    memory.save_context({\"input\": user_q}, {\"output\": answer})\n",
    "    Path(\"output.txt\").write_text(answer, encoding=\"utf-8\")\n",
    "    print(\"Written to output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1f13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG latency: 0.01 s\n",
      "\n",
      "First token latency: 0.51 s\n",
      "\n",
      "I don't have personal preferences when it comes to food, as I am a computer program. However, I can appreciate a wide variety of dishes from different cultures, such as sushi, pasta, tacos, and curry. Enjoying food is part of maintaining a balanced and healthy lifestyle.\n",
      "\n",
      "Tokens per second: 4.78\n",
      "Written to output.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    stream_chat_from_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82b084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
